{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:40:48.719298Z",
          "iopub.status.busy": "2025-06-15T00:40:48.719021Z",
          "iopub.status.idle": "2025-06-15T00:40:48.723769Z",
          "shell.execute_reply": "2025-06-15T00:40:48.723007Z",
          "shell.execute_reply.started": "2025-06-15T00:40:48.719275Z"
        },
        "id": "k7r1ALRLbIA9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments, Trainer, EarlyStoppingCallback, AutoTokenizer, DefaultDataCollator\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "f-dzjZm3mj5J"
      },
      "outputs": [],
      "source": [
        "#!pip install -U bitsandbytes accelerate transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:45:50.272713Z",
          "iopub.status.busy": "2025-06-15T00:45:50.272409Z",
          "iopub.status.idle": "2025-06-15T00:46:04.918610Z",
          "shell.execute_reply": "2025-06-15T00:46:04.917920Z",
          "shell.execute_reply.started": "2025-06-15T00:45:50.272691Z"
        },
        "trusted": true,
        "id": "VHK46_Kjmj5J",
        "outputId": "d1ed7227-88a3-4701-af8d-89783fdd8536"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancescobrigante\u001b[0m (\u001b[33mfrancescobrigante_s_projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/kaggle/working/wandb/run-20250615_004557-73rjkyp2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica/runs/73rjkyp2' target=\"_blank\">Final train with rank=64 and alpha=32 pt2</a></strong> to <a href='https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica' target=\"_blank\">https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica/runs/73rjkyp2' target=\"_blank\">https://wandb.ai/francescobrigante_s_projects/Digital%20Self-Replica/runs/73rjkyp2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "WB_KEY = user_secrets.get_secret(\"WB_KEY\")\n",
        "\n",
        "wandb.login(key=WB_KEY)\n",
        "run = wandb.init(project=\"Digital Self-Replica\", job_type=\"Training\", name=\"Final train with rank=64 and alpha=32 pt2\")\n",
        "if (HF_TOKEN == None):\n",
        "    raise ValueError(\"HF_TOKEN is not set\")\n",
        "login(token=HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfOzfMKvmj5K"
      },
      "source": [
        "### Setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:46:04.920186Z",
          "iopub.status.busy": "2025-06-15T00:46:04.919980Z",
          "iopub.status.idle": "2025-06-15T00:46:04.928412Z",
          "shell.execute_reply": "2025-06-15T00:46:04.927677Z",
          "shell.execute_reply.started": "2025-06-15T00:46:04.920169Z"
        },
        "id": "YZ-EagcCbIA-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 4 bit quantization\n",
        "# could be further increased to 8b for more precision\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration for Qwen model architecture\n",
        "lora_config = LoraConfig(\n",
        "    r=64,                       #rank of the added low-rank matrices\n",
        "    lora_alpha=32,              #generally 2*r\n",
        "    target_modules=[            #modules where LoRA is applied\n",
        "        \"q_proj\",               # query, key, value, output projection layers in the self-attention\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",            # gate, up, down are part of the FFNN in the model\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:46:04.929593Z",
          "iopub.status.busy": "2025-06-15T00:46:04.929326Z",
          "iopub.status.idle": "2025-06-15T00:46:46.586402Z",
          "shell.execute_reply": "2025-06-15T00:46:46.585869Z",
          "shell.execute_reply.started": "2025-06-15T00:46:04.929570Z"
        },
        "id": "lRra3TBZbIA-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:46:46.588120Z",
          "iopub.status.busy": "2025-06-15T00:46:46.587843Z",
          "iopub.status.idle": "2025-06-15T00:46:48.445281Z",
          "shell.execute_reply": "2025-06-15T00:46:48.444708Z",
          "shell.execute_reply.started": "2025-06-15T00:46:46.588099Z"
        },
        "trusted": true,
        "id": "EAa1tuzsmj5L"
      },
      "outputs": [],
      "source": [
        "# preparing model for LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "# training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./francesco_lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,      # effective batch size = per_device_train_batch_size * gradinet_accumulation_steps\n",
        "    per_device_eval_batch_size=6,\n",
        "    eval_accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=3e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,            #lower loss is better\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=1,\n",
        "    disable_tqdm=False,\n",
        "    report_to=[\"wandb\"],                                # W&B logging\n",
        "    label_names=[\"labels\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:46:48.446155Z",
          "iopub.status.busy": "2025-06-15T00:46:48.445909Z",
          "iopub.status.idle": "2025-06-15T00:46:49.551796Z",
          "shell.execute_reply": "2025-06-15T00:46:49.550662Z",
          "shell.execute_reply.started": "2025-06-15T00:46:48.446134Z"
        },
        "id": "0TzB-MGsbIA-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "# loading datasets\n",
        "tokenized_train = load_from_disk('/kaggle/input/tok-datasets/datasets/tokenized_train')\n",
        "tokenized_val = load_from_disk('/kaggle/input/tok-datasets/datasets/tokenized_val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI3g9j-Cmj5M"
      },
      "source": [
        "### Actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-15T00:47:02.782377Z",
          "iopub.status.busy": "2025-06-15T00:47:02.782090Z",
          "iopub.status.idle": "2025-06-15T00:47:02.812629Z",
          "shell.execute_reply": "2025-06-15T00:47:02.812021Z",
          "shell.execute_reply.started": "2025-06-15T00:47:02.782356Z"
        },
        "id": "PtZ4cWX3bIA_",
        "outputId": "9fc70b73-7bc2-4627-86ec-2d874ea5758b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 161,480,704 || all params: 7,777,097,216 || trainable%: 2.0764\n"
          ]
        }
      ],
      "source": [
        "# print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# add early stopping\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
        "trainer.add_callback(early_stopping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "y_58tSs7mj5N"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-15T00:47:08.536247Z",
          "iopub.status.busy": "2025-06-15T00:47:08.535957Z",
          "iopub.status.idle": "2025-06-15T06:08:57.829242Z",
          "shell.execute_reply": "2025-06-15T06:08:57.828673Z",
          "shell.execute_reply.started": "2025-06-15T00:47:08.536224Z"
        },
        "trusted": true,
        "id": "s9pJ3ypEmj5N",
        "outputId": "c5145610-4ee3-46fe-81db-6b86df7debc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='1206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 900/1206 5:20:51 < 5:29:28, 0.02 it/s, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.545500</td>\n",
              "      <td>3.456467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.409100</td>\n",
              "      <td>3.417243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.238200</td>\n",
              "      <td>3.496536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=900, training_loss=0.8073342344495985, metrics={'train_runtime': 19306.2088, 'train_samples_per_second': 1.997, 'train_steps_per_second': 0.062, 'total_flos': 3.199471712128205e+17, 'train_loss': 0.8073342344495985, 'epoch': 2.241443683883012})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint=\"./francesco_lora/checkpoint-600\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7664013,
          "sourceId": 12168446,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}